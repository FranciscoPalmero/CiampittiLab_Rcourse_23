
# Packages
```{r message=FALSE, warning=FALSE}
easypackages::libraries("tidyverse", "car", "agricolae")

```


# ---------------------------------
# LINEAR REGRESION

# 1) Example 1: Linear Regression
# 1.1) Data
```{r}
wheat <- read.csv("../Data/wheat_yield.csv")
```

# 1.2) Quick view
- Numerically
```{r}
glimpse(wheat)
summary(wheat)
```

- Graphically
```{r}
names(wheat)
wheat %>% 
  ggplot() +
  geom_point(aes(N_rate, yield_kg.ha)) +
  scale_x_continuous(limits = c(40,100),
                     expand = c(0,0)) +
  scale_y_continuous(limits = c(0,3200),
                     expand = c(0,0))
```

# 1.3) Linear model
# 1.3.1) Fit the model
- lm() function
```{r}
names(wheat)
wheat_yield <- lm(yield_kg.ha ~ N_rate, data = wheat)

wheat_yield#$

```

- Test hypothesis
```{r}
summary(wheat_yield)
```


# 1.3.2) Checking assumptions
- Getting predicted values
```{r}
# Add predicted values into the data frame
# - Tidyverse option
wheat <- wheat %>% 
  mutate(pred.lm = fitted(wheat_yield))

# - R base options
wheat$pred.lm <- wheat_yield$fitted.values
# wheat$pred.lm <- fitted(wheat_yield)
```

- Getting the residuals
```{r}
# - Optiont 1 -> residuals = observed - predicted
wheat$netp_g.m2.y - wheat$pred.lm

# - Option 2 -> use residuals() function or the object created for the model
wheat$res.lm <- residuals(wheat_yield)
wheat$res.lm <- wheat_yield$residuals
```

# 1.3.2.1) Normality of the residuals
Q-Q plot
```{r}
wheat %>% 
  ggplot() +
  stat_qq(aes(sample = res.lm)) +
  stat_qq_line(aes(sample = res.lm))

```

# 1.3.2.2) Homocedasticity (homogeinity of variance)
- Residuals vs Fitted
```{r}
wheat %>% 
  ggplot() +
  geom_point(aes(pred.lm, res.lm)) +
  geom_hline(yintercept = 0) +
  labs(y = expression(Residuals), 
       x = expression(Fitted))
```



```{r}
wheat %>% 
  ggplot() +
  geom_point(aes(N_rate, yield_kg.ha)) +
  scale_x_continuous(limits = c(40,100)) +
  scale_y_continuous(limits = c(0,3200))
```





# 2) Example 1: Linear Regression
# 2.1) Data
```{r}
water <- read.csv("../Data/water_pasture.csv")
```

# 2.2) Quick view
- Numerically
```{r}
glimpse(water)
summary(water)
```

- Graphically
```{r}
water %>% 
  ggplot() +
  geom_point(aes(pp_anual_mm, netp_g.m2.y)) +
  scale_x_continuous(limits = c(0,1200),
                     expand = c(0,0)) +
  scale_y_continuous(limits = c(0,1000), 
                     expand = c(0,0)) +
  coord_cartesian(xlim = c(0,1300))
```


# 2.3) Linear model
# 2.3.1) Fit the model
- lm() function
```{r}
names(water)
past_prod <- lm(netp_g.m2.y ~ pp_anual_mm, data = water)

past_prod#$

```

- Test hypothesis
```{r}
summary(past_prod)
```


# 2.3.2) Checking assumptions
- Getting predicted values
```{r}
# Add predicted values into the data frame
# - Tidyverse option
water <- water %>% 
  mutate(pred.lm = fitted(past_prod))

# - R base options
water$pred.lm <- past_prod$fitted.values
# water$pred.lm <- fitted(past_prod)
```

- Getting the residuals
```{r}
# - Optiont 1 -> residuals = observed - predicted
water$netp_g.m2.y - water$pred.lm

# - Option 2 -> use residuals() function or the object created for the model
water$res.lm <- residuals(past_prod)
water$res.lm <- past_prod$residuals
```

# 2.3.2.1) Normality of the residuals
Q-Q plot
```{r}
water %>% 
  ggplot() +
  stat_qq(aes(sample = res.lm)) +
  stat_qq_line(aes(sample = res.lm))

```

# 2.3.2.2) Homocedasticity (homogeinity of variance)
- Residuals vs Fitted
```{r}
water %>% 
  ggplot() +
  geom_point(aes(pred.lm, res.lm)) +
  geom_hline(yintercept = 0) +
  labs(y = expression(Residuals), 
       x = expression(Fitted))
```



# 3) Example: Quadratic Model
# 3.1) Data
```{r}
yield_dens <- readxl::read_excel("../Data/dens_yield.xlsx")

yield_dens
```

# 3.2) Quick View
- Numerically
```{r}
glimpse(yield_dens)
summary(yield_dens)
```

- Graphically
```{r}
yield_dens %>% 
  ggplot() +
  geom_point(aes(pl_m2, 
                 yield_t.ha),
             shape = 21, color = "black",
             fill = "#117a65", size = 3,
             alpha = 0.5
             ) +
  coord_cartesian(xlim = c(0,13), 
                  ylim = c(7,12.5)) +
  scale_x_continuous(expand = c(0,0)) +
  labs(y = expression(Yield~(t~ha^-1)),
       x = expression(Density~(plants~m^-2))) +
  theme_bw()
```

# 3.3) Linear Model
# 3.3.1) Fit the model
a) Linear Model
```{r}
names(yield_dens)
m1 <- lm(yield_t.ha ~ pl_m2, data = yield_dens)

summary(m1)
```

a.1) Plot
```{r}
yield_dens %>% 
  ggplot() +
  geom_point(aes(pl_m2, 
                 yield_t.ha),
             shape = 21, color = "black",
             fill = "#117a65", size = 3,
             alpha = 0.5
             ) +
  coord_cartesian(xlim = c(0,13), 
                  ylim = c(7,12.5)) +
  scale_x_continuous(expand = c(0,0)) +
  labs(y = expression(Yield~(t~ha^-1)),
       x = expression(Density~(plants~m^-2))) +
  theme_bw() +
  geom_line(aes(x = yield_dens$pl_m2,
                y = m1$fitted.values),
            size = 1, color = "blue")

```



b) Quadratic Model
```{r}
names(yield_dens)
m2 <- lm(yield_t.ha ~ pl_m2 + I(pl_m2^2), data = yield_dens)

summary(m2)
```

b.1) Plot
```{r}
yield_dens %>% 
  ggplot() +
  geom_point(aes(pl_m2, 
                 yield_t.ha),
             shape = 21, color = "black",
             fill = "#117a65", size = 3,
             alpha = 0.5
             ) +
  coord_cartesian(xlim = c(0,13), 
                  ylim = c(7,12.5)) +
  scale_x_continuous(expand = c(0,0)) +
  labs(y = expression(Yield~(t~ha^-1)),
       x = expression(Density~(plants~m^-2))) +
  theme_bw() +
  geom_line(aes(x = yield_dens$pl_m2,
                y = m2$fitted.values),
            size = 1, color = "blue")
```


# 3.3.2) Checking the assumptions
a) Linear model
```{r}
plot(m1)
```


b) Quadratic model
```{r}
plot(m2)
```

# 3.4) Selecting the best model
```{r}
AIC(m1, m2)
BIC(m1, m2)
```


# ---------------------------------
# ANOVA

# 1) Data
```{r}
df_yield <- read.csv("../Data/N_initiative/N_rate_yield.csv")

df_yield %>% 
  filter(Site_Prod == "High") %>% 
  distinct(Site)
```

# 2) Quick view
```{r}
glimpse(df_yield)
summary(df_yield)
```

# 3) Getting the final data
```{r}
df_yield <- df_yield %>%
  mutate(across(where(is.numeric), as.character),
         across(everything(),
                ~na_if(., ".")),
         across(.cols = c(Trial, Year, 
                          Block, Plant_N),
                ~as.factor(.)),
         across(Yield_SY, as.numeric)
         ) %>% 
  rename(N_rate = Plant_N)

summary(df_yield)
```

```{r}
MC <- df_yield %>%   
  filter(Site == "MasonCity") %>%
  group_by(Block, N_rate) %>% 
  summarise(Yield_SY = mean(Yield_SY)) %>% 
  ungroup()
  # group_by(Block, N_rate) %>% 
  # count()


# MC <- df_yield %>% 
#   filter(Site == "MasonCity") %>% 
#   filter(N_rate %in% c("0", "120", "160", 
#                        "200", "240", "280")) #%>% 
  # group_by(Block, N_rate) %>% 
  # count()

summary(MC)
```


```{r}
MC %>% 
  ggplot() +
  geom_point(aes(as.numeric(N_rate), 
                 Yield_SY, color = Block))


MC %>% 
  ggplot() +
  geom_point(aes(as.numeric(Block), 
                 Yield_SY, color = N_rate))
```



```{r}
MC %>% 
  ggplot() +
  geom_boxplot(aes(N_rate, Yield_SY)) +
  coord_cartesian(ylim = c(0,15000))
```

# 4) Fitting the model
```{r}
MC

# Null model
m1 <- aov(Yield_SY ~ 1, data = MC)
summary(m1)
m1$fitted.values
mean(MC$Yield_SY)

# Model representing that corn grain yield depends on the N rate

# lm(Yield_SY ~ N_rate+Block, data = MC)
# summary(lm(Yield_SY ~ N_rate+Block, data = MC))
# anova(lm(Yield_SY ~ N_rate+Block, data = MC))

m2 <- aov(Yield_SY ~ N_rate+Block, data = MC)
summary(m2)

```

Which model is better?
```{r}
AIC(m1, m2)
BIC(m1, m2)

```



# 5) Checking assumptions
- Visually
```{r}
plot(m2)

# ggplot() +
#   geom_point(aes(y = rstandard(m1), 
#                  x = m1$fitted.values)) +
#   geom_hline(yintercept = 0)

```

- Formally
```{r}
# The assumptions of a RCBD is that the factor effects do not interact with the blocks. So, the variance of the levels inside the factor does not depend on the block.

# - Testing homogeneity of variance

# Levene's test is less sensitive to departures from normality that the Barlett's test.
car::leveneTest(Yield_SY~N_rate, data = MC)
#car::leveneTest(Yield_SY~Block, data = MC)

# Testing for normality
shapiro.test(m2$residuals)
```


# 6) Different sums of squares
```{r}
anova(m2)
car::Anova(m2, type="II")
car::Anova(m2, type="III")
```



# 7) Pairwise comparisons
```{r}
agricolae::HSD.test(m2, trt = c("N_rate"), alpha = 0.05,
                    unbalanced = FALSE,
                    console = TRUE)

plot(agricolae::HSD.test(m2, trt = c("N_rate"), alpha = 0.05,
                         unbalanced = FALSE,
                         console = TRUE))

agricolae::LSD.test(m2, trt = c("N_rate"), 
                    DFerror = m2$df.residual, 
                    MSerror = 593071,
                    alpha = 0.05, 
                    p.adj = "bonferroni",
                    console = TRUE)

```






